{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density-Based Clustering\n",
    "### Hannah Valenty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charbel's comment on the process of predetermining clusters and that method's lack of flexibility piqued my interest to look into other input options. This pointed me in the direction of agglomerative clustering and more specifically, Ward hierarchical clustering using linkage distance. The input parameter of distance is a shift from the number of clusters used in kmeans and other common clustering approaches.\n",
    "\n",
    "However, the Ward clustering was not performing well, so I pivoted to DBSCAN clustering. Density-Based Spatial Clustering of Applications with Noise clusters together those points that are close to each other based on any distance metric and a minimum number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preface\n",
    "The methods to translate images, load yolo data, and select bounding boxes are from Charbel's `clustering.ipynb` document. This pipeline was immensely helpful for allowing me to smoothly create and train a supplemental clustering algorithm. The sections taken from this document will be labelled as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Images to Start (`clustering.ipynb`)\n",
    "\n",
    "To start, we need to register images using the `utilities/conversion/apply_homography_to_labels.ipynb` notebook. This should be run before running this notebook. This notebook is built on the assumption that the `data/registered_images` directory has been created and populated. Additionally it assumes that the `data/yolo_data.json` file is created. Both of these are created in the referenced notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages\n",
    "Import libraries for analysis, with change in sklearn.cluster from KMeans to AgglomerativeClustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by loading YOLO data (`clustering.ipynb`)\n",
    "To start I want to bring in the YOLO formatted data for each sheet and I can additionally load the respective images. As mentioned above you must have ran the `utilities/conversion/apply_homography_to_labels.ipynb` notebook to generate this YOLO data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 sheets in yolo_data.json\n"
     ]
    }
   ],
   "source": [
    "# Load yolo_data.json\n",
    "PATH_TO_YOLO_DATA = '../../data/yolo_data.json'\n",
    "PATH_TO_REGISTERED_IMAGES = '../../data/registered_images'\n",
    "UNIFIED_IMAGE_PATH = '../../data/unified_intraoperative_preoperative_flowsheet_v1_1_front.png'\n",
    "with open(PATH_TO_YOLO_DATA) as json_file:\n",
    "    yolo_data = json.load(json_file)\n",
    "\n",
    "# See how many intraoperative images are registered\n",
    "print(f\"Found {len(yolo_data)} sheets in yolo_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select relevant bounding boxes from the blood pressure and HR zone. \n",
    "\n",
    "Start by defining functions to convert YOLO bounding box format to pixels (to see if the bounding box is within region of interest). Then create a function that allows you to select ROI and returns a list of bounding boxes within this ROI.\n",
    "\n",
    "Also identifies which boxes are for timestamps (top-right) or numeric values of mmHg and bpm (bottom-left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLO_to_pixels(x_center, y_center, width, height, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Convert YOLO bounding box format to pixel coordinates\n",
    "\n",
    "    Args:\n",
    "        x_center: float, x center of the bounding box\n",
    "        y_center: float, y center of the bounding box\n",
    "        width: float, width of the bounding box\n",
    "        height: float, height of the bounding box\n",
    "        image_width: int, width of the image\n",
    "        image_height: int, height of the image\n",
    "\n",
    "    Returns:\n",
    "        A single tuple with the following values:\n",
    "            x_min: int, minimum x coordinate of the bounding box in pixels\n",
    "            y_min: int, minimum y coordinate of the bounding box in pixels\n",
    "            x_max: int, maximum x coordinate of the bounding box in pixels\n",
    "            y_max: int, maximum y coordinate of the bounding box in pixels\n",
    "    \"\"\"\n",
    "    x_min = int((float(x_center) * image_width) - (width * image_width) / 2)\n",
    "    y_min = int((float(y_center) * image_height) - (height * image_height) / 2)\n",
    "    x_max = int((float(x_center) * image_width) + (width * image_width) / 2)\n",
    "    y_max = int((float(y_center) * image_height) + (height * image_height) / 2)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "# Function to determine whether a point is above or below the diagonal line\n",
    "def is_point_in_above(x_center, y_center, m, b):\n",
    "    \"\"\"\n",
    "    Determine if a point is above or below the diagonal line y = mx + b.\n",
    "    For our purposes we use it to check if a bounding box is in the top-right region -- meaning time labels.\n",
    "\n",
    "    Args:\n",
    "        x_center: float, x coordinate of the point\n",
    "        y_center: float, y coordinate of the point\n",
    "        m: float, slope of the diagonal line\n",
    "        b: float, intercept of the diagonal line\n",
    "    \n",
    "    Returns:\n",
    "        bool, True if the point is above the line, False otherwise\n",
    "    \"\"\"\n",
    "    # Calculate the y value on the line for the given x_center\n",
    "    y_line = m * x_center + b\n",
    "    return y_center > y_line\n",
    "\n",
    "\n",
    "\n",
    "def select_relevant_bounding_boxes(sheet_data: List[str], path_to_image: Path, show_images: bool = False) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Given sheet data for bounding boxes in YOLO format, display the image and allow the user to select a region of interest (ROI).\n",
    "    Identify bounding boxes that are within the selected region and draw rectangles around them. \n",
    "    Return the bounding boxes that are within the selected region split into two lists: time labels and numerical values.\n",
    "\n",
    "    Args:\n",
    "        sheet_data: List of bounding boxes in YOLO format.\n",
    "        path_to_image: Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of Lists of string representations of bounding boxes that are within the selected region, in YOLO format.\n",
    "        The first list contains bounding boxes in the top-right region -- representing time labels.\n",
    "        The second list contains bounding boxes in the bottom-left region -- representing numerical values for mmHg and bpm.\n",
    "            (bounding_boxes_time, bounding_boxes_numbers)\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(path_to_image)\n",
    "\n",
    "    # Display the image and allow the user to select a ROI\n",
    "    resized_image = cv2.resize(image, (800, 600))\n",
    "    roi = cv2.selectROI(\"Select Region of Interest\", resized_image)\n",
    "    print(f\"ROI selected: {roi}\")\n",
    "\n",
    "    # Unpack ROI\n",
    "    x, y, w, h = roi\n",
    "    print(f\"Selected region: x={x}, y={y}, w={w}, h={h}\")\n",
    "\n",
    "    # Calculate the coordinates of the top-left and bottom-right corners of the selected region\n",
    "    x_top_left = x\n",
    "    y_top_left = y\n",
    "    x_bottom_right = x + w\n",
    "    y_bottom_right = y + h\n",
    "\n",
    "    # Draw the diagonal line of the selected region from top-left to bottom-right\n",
    "    cv2.line(resized_image, (x_top_left, y_top_left), (x_bottom_right, y_bottom_right), (0, 255, 0), 1)\n",
    "    # Calculate the slope (m) and intercept (b) of the diagonal line.\n",
    "    # This will allow us to determine if a bounding box is in the top-right region or bottom-left region\n",
    "    # Top-right region is where time labels are located\n",
    "    # Bottom-left region is where numerical values for mmHg and bpm are located\n",
    "    m = (y_bottom_right - y_top_left) / (x_bottom_right - x_top_left)\n",
    "    b = y_top_left - m * x_top_left\n",
    "\n",
    "    # List of bounding boxes in the top-right and bottom-left regions\n",
    "    bounding_boxes_time = []\n",
    "    bounding_boxes_numbers = []\n",
    "\n",
    "    # Process the bounding boxes\n",
    "    for bounding_box in sheet_data:\n",
    "        # Bounding boxes are in YOLO format; convert them to pixels\n",
    "        identifier, x_center, y_center, bb_width, bb_height = bounding_box.split(' ') # Identifier is the value in the bounding box, we don't need that here\n",
    "        x_min, y_min, x_max, y_max  = YOLO_to_pixels(float(x_center), float(y_center), float(bb_width), float(bb_height), 800, 600)\n",
    "        \n",
    "        # Check if the bounding box is within the selected region\n",
    "        if x_min >= x and y_min >= y and x_max <= x + w and y_max <= y + h:\n",
    "            # Calculate the center of the bounding box\n",
    "            x_center_bb = (x_min + x_max) / 2\n",
    "            y_center_bb = (y_min + y_max) / 2\n",
    "            \n",
    "            # If we want to generalize this function we can add the option to disregard the diagonal line\n",
    "\n",
    "            # Determine if the bounding box center is in the top-right region\n",
    "            if is_point_in_above(x_center_bb, y_center_bb, m, b):\n",
    "                # Bounding box is in the top-right region\n",
    "                cv2.rectangle(resized_image, (x_min, y_min), (x_max, y_max), (255, 255, 0), 1)\n",
    "                bounding_boxes_numbers.append(bounding_box)\n",
    "            else:\n",
    "                # Bounding box is in the bottom-left region\n",
    "                cv2.rectangle(resized_image, (x_min, y_min), (x_max, y_max), (255, 0, 255), 1)\n",
    "                bounding_boxes_time.append(bounding_box)\n",
    "\n",
    "    # Close all OpenCV windows, always do this or it will annoyingly not go away\n",
    "    # You can also manually quit out with ESC key.\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # If we are showing the images, display the image with the selected region and bounding boxes\n",
    "    # Bounding boxes in the top-right region (time) are in one color while those in the bottom left (numerical) are in another\n",
    "    if show_images:\n",
    "        # Display the image with the selected region and bounding boxes\n",
    "        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "        resized_image = Image.fromarray(resized_image)\n",
    "        resized_image.show()\n",
    "\n",
    "    # Return a tuple of bounding boxes in the top-right and bottom-left regions\n",
    "    return (bounding_boxes_time, bounding_boxes_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a function for Ward hierarchical clustering using linkage distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clustering(bounding_boxes: List[str], defined_eps: float, min_samples: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Cluster bounding boxes using Ward hierarchical clustering algorithm with linkage distance.\n",
    "\n",
    "    Args:\n",
    "        bounding_boxes: List of bounding boxes in YOLO format.\n",
    "        defined_eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "        min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "    Returns:\n",
    "        List of cluster labels.\n",
    "    \"\"\"\n",
    "    # Convert to a NumPy array (using only x_center and y_center)\n",
    "    data = np.array([[float(box.split(' ')[1]), float(box.split(' ')[2])] for box in bounding_boxes])\n",
    "\n",
    "    # DBSCAN\n",
    "    scan = DBSCAN(eps=defined_eps, min_samples=min_samples)\n",
    "    labels = scan.fit_predict(data)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create a result dictionary that we can save as a JSON file to analyze performance. (`clustering.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_dictionary(labels: List[str], bounding_boxes: List[str]) -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Create a dictionary with cluster labels as keys and lists of bounding boxes as values.\n",
    "\n",
    "    Args:\n",
    "        labels: List of cluster labels.\n",
    "        bounding_boxes: List of bounding boxes in YOLO format.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with cluster labels as keys and bounding box values as values.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store labelled elements\n",
    "    label_dict = {}\n",
    "\n",
    "    # Iterate over both lists\n",
    "    for label, element in zip(labels, bounding_boxes):\n",
    "        label = int(label)\n",
    "        element = str(element)\n",
    "        if label not in label_dict:\n",
    "            # Create a new list for this label if it doesn't exist\n",
    "            label_dict[label] = []\n",
    "        # Append the element to the corresponding label list\n",
    "        label_dict[label].append(element)\n",
    "\n",
    "    # Sort the lists in the dictionary by x_center\n",
    "    for key in label_dict:\n",
    "        label_dict[key] = sorted(label_dict[key], key=lambda x: float(x.split(' ')[1]))\n",
    "        label_dict[key] = [element.split(' ')[0] for element in label_dict[key]]\n",
    "        # Turn list of strings into a string\n",
    "        label_dict[key] = int(''.join(label_dict[key]))\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use these functions to get the relevant bounding boxes for clustering. (`clustering.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet: RC_0001_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0001_intraoperative.JPG\n",
      "ROI selected: (103, 221, 635, 205)\n",
      "Selected region: x=103, y=221, w=635, h=205\n",
      "Sheet: RC_0002_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0002_intraoperative.JPG\n",
      "ROI selected: (104, 221, 634, 205)\n",
      "Selected region: x=104, y=221, w=634, h=205\n",
      "Sheet: RC_0003_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0003_intraoperative.JPG\n",
      "ROI selected: (101, 221, 636, 204)\n",
      "Selected region: x=101, y=221, w=636, h=204\n",
      "Sheet: RC_0004_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0004_intraoperative.JPG\n",
      "ROI selected: (101, 219, 636, 206)\n",
      "Selected region: x=101, y=219, w=636, h=206\n",
      "Sheet: RC_0005_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0005_intraoperative.JPG\n",
      "ROI selected: (101, 219, 640, 207)\n",
      "Selected region: x=101, y=219, w=640, h=207\n",
      "Sheet: RC_0006_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0006_intraoperative.JPG\n",
      "ROI selected: (101, 219, 640, 207)\n",
      "Selected region: x=101, y=219, w=640, h=207\n",
      "Sheet: RC_0007_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0007_intraoperative.JPG\n",
      "ROI selected: (104, 217, 633, 207)\n",
      "Selected region: x=104, y=217, w=633, h=207\n",
      "Sheet: RC_0008_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0008_intraoperative.JPG\n",
      "ROI selected: (105, 223, 628, 201)\n",
      "Selected region: x=105, y=223, w=628, h=201\n",
      "Sheet: RC_0009_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0009_intraoperative.JPG\n",
      "ROI selected: (103, 221, 633, 205)\n",
      "Selected region: x=103, y=221, w=633, h=205\n",
      "Sheet: RC_0010_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0010_intraoperative.JPG\n",
      "ROI selected: (103, 221, 635, 205)\n",
      "Selected region: x=103, y=221, w=635, h=205\n",
      "Sheet: RC_0011_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0011_intraoperative.JPG\n",
      "ROI selected: (102, 218, 636, 208)\n",
      "Selected region: x=102, y=218, w=636, h=208\n",
      "Sheet: RC_0012_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0012_intraoperative.JPG\n",
      "ROI selected: (102, 218, 632, 205)\n",
      "Selected region: x=102, y=218, w=632, h=205\n",
      "Sheet: RC_0013_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0013_intraoperative.JPG\n",
      "ROI selected: (101, 219, 633, 204)\n",
      "Selected region: x=101, y=219, w=633, h=204\n",
      "Sheet: RC_0014_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0014_intraoperative.JPG\n",
      "ROI selected: (101, 219, 637, 207)\n",
      "Selected region: x=101, y=219, w=637, h=207\n",
      "Sheet: RC_0015_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0015_intraoperative.JPG\n",
      "ROI selected: (104, 221, 634, 205)\n",
      "Selected region: x=104, y=221, w=634, h=205\n",
      "Sheet: RC_0016_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0016_intraoperative.JPG\n",
      "ROI selected: (104, 221, 631, 207)\n",
      "Selected region: x=104, y=221, w=631, h=207\n",
      "Sheet: RC_0017_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0017_intraoperative.JPG\n",
      "ROI selected: (105, 221, 630, 203)\n",
      "Selected region: x=105, y=221, w=630, h=203\n",
      "Sheet: RC_0018_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0018_intraoperative.JPG\n",
      "ROI selected: (104, 220, 631, 204)\n",
      "Selected region: x=104, y=220, w=631, h=204\n",
      "Sheet: RC_0019_intraoperative.JPG\n",
      "Full image path: ../../data/registered_images\\RC_0019_intraoperative.JPG\n",
      "ROI selected: (104, 220, 632, 204)\n",
      "Selected region: x=104, y=220, w=632, h=204\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all images and their bounding boxes\n",
    "for sheet, bounding_boxes in yolo_data.items():\n",
    "    print(f\"Sheet: {sheet}\")\n",
    "    full_image_path = os.path.join(PATH_TO_REGISTERED_IMAGES, sheet)\n",
    "    print(f\"Full image path: {full_image_path}\")\n",
    "\n",
    "    # Call the analyze_sheet function with data from the loop\n",
    "    time_bounding_boxes, number_bounding_boxes = select_relevant_bounding_boxes(bounding_boxes, full_image_path)\n",
    "\n",
    "    # Now we need to cluster the bounding boxes that pertain to the same multi-digit number\n",
    "    # Eps chosen based on width units of bounding boxes -- on average 0.0042-0.0045 wide\n",
    "    time_labels = dbscan_clustering(time_bounding_boxes, defined_eps=0.01, min_samples=1)\n",
    "    number_labels = dbscan_clustering(number_bounding_boxes, defined_eps=0.01, min_samples=2)\n",
    "\n",
    "    # Create an image object\n",
    "    image: Image = Image.open(full_image_path)\n",
    "    image_width, image_height = image.size\n",
    "\n",
    "    label_color_map = {}\n",
    "    for i, label in enumerate(time_labels):\n",
    "\n",
    "        # Get the bounding box\n",
    "        bounding_box = time_bounding_boxes[i]\n",
    "        value, x_center, y_center, width, height = bounding_box.split(' ')\n",
    "        x_min, y_min, x_max, y_max = YOLO_to_pixels(float(x_center), float(y_center), float(width), float(height), image_width, image_height)\n",
    "\n",
    "        # Draw bounding boxes on the image\n",
    "        generate_color = lambda: \"#%06x\" % random.randint(0, 0xFFFFFF)\n",
    "\n",
    "        # If the label is not in the color map, generate a new color\n",
    "        if label not in label_color_map:\n",
    "            label_color_map[label] = generate_color()\n",
    "\n",
    "        # Open the image\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        box = [\n",
    "            x_min,\n",
    "            y_min,\n",
    "            x_max,\n",
    "            y_max,\n",
    "        ]\n",
    "        draw.rectangle(box, outline=label_color_map[label], width=3)\n",
    "\n",
    "    # Save the image with the bounding boxes to the kmeans_clustered_images folder\n",
    "    image.save(f'../../data/kmeans_clustered_images/time/{sheet}')\n",
    "\n",
    "    # Save the clustered bounding boxes to a JSON file\n",
    "    with open(f'../../data/kmeans_clustered_images/results/time/{sheet}.json', 'w') as f:\n",
    "        json.dump(create_result_dictionary(time_labels, time_bounding_boxes), f)\n",
    "\n",
    "    # Create an image object\n",
    "    image: Image = Image.open(full_image_path)\n",
    "    image_width, image_height = image.size\n",
    "\n",
    "    label_color_map = {}\n",
    "    for i, label in enumerate(number_labels):\n",
    "\n",
    "        # Get the bounding box\n",
    "        bounding_box = number_bounding_boxes[i]\n",
    "        value, x_center, y_center, width, height = bounding_box.split(' ')\n",
    "        x_min, y_min, x_max, y_max = YOLO_to_pixels(float(x_center), float(y_center), float(width), float(height), image_width, image_height)\n",
    "\n",
    "        # Draw bounding boxes on the image\n",
    "        generate_color = lambda: \"#%06x\" % random.randint(0, 0xFFFFFF)\n",
    "\n",
    "\n",
    "        # If the label is not in the color map, generate a new color\n",
    "        if label not in label_color_map:\n",
    "            label_color_map[label] = generate_color()\n",
    "\n",
    "        # Open the image\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        box = [\n",
    "            x_min,\n",
    "            y_min,\n",
    "            x_max,\n",
    "            y_max,\n",
    "        ]\n",
    "        draw.rectangle(box, outline=label_color_map[label], width=3)\n",
    "\n",
    "    # Save the image with the bounding boxes to the kmeans_clustered_images folder\n",
    "    image.save(f'../../data/kmeans_clustered_images/number/{sheet}')\n",
    "\n",
    "    # Save the clustered bounding boxes to a JSON file\n",
    "    with open(f'../../data/kmeans_clustered_images/results/number/{sheet}.json', 'w') as f:\n",
    "        json.dump(create_result_dictionary(number_labels, number_bounding_boxes), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze accuracy (`clustering.ipynb`)\n",
    "\n",
    "Below we use assumptions on what we know the labels should represent in both the time and number groups. We check that these values are present within clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time labels: 794 correct clusters, 0 incorrect clusters. The accuracy is 100.00%\n",
      "Number labels: 380 correct clusters, 3 incorrect clusters. The accuracy is 99.22%\n"
     ]
    }
   ],
   "source": [
    "# Since this work is done above, we can simply read in from the JSON files created in the previous step and work from there.\n",
    "\n",
    "# Paths to the JSON files\n",
    "PATH_TO_KMEANS_RESULTS = '../../data/kmeans_clustered_images/results'\n",
    "TIME_JSON = os.path.join(PATH_TO_KMEANS_RESULTS, 'time')\n",
    "NUMBER_JSON = os.path.join(PATH_TO_KMEANS_RESULTS, 'number')\n",
    "\n",
    "time_wrong_clusters_count = 0\n",
    "time_correct_clusters_count = 0\n",
    "number_wrong_clusters_count = 0\n",
    "number_correct_clusters_count = 0\n",
    "# Iterate over all images and their bounding boxes\n",
    "for sheet, bounding_boxes in yolo_data.items():\n",
    "    expected_time_values = [0,5,10,15,20,25,30,35,40,45,50,55,0,5,10,15,20,25,30,35,40,45,50,55,0,5,10,15,20,25,30,35,40,45,50,55,0,5,10,15,20,25]\n",
    "    expected_number_values = [30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220]\n",
    "\n",
    "    # Load JSON\n",
    "    with open(os.path.join(TIME_JSON, f'{sheet}.json')) as f:\n",
    "        time_clusters = json.load(f)\n",
    "    \n",
    "    # Each cluster contains the number (integer) that the cluster represents\n",
    "    # We know what integers should be represented in the time labels, lets check that they are all there.\n",
    "    # Keep track of any false positives (new clusters that don't exist) or negatives (missing clusters)\n",
    "    for cluster, value in time_clusters.items():\n",
    "        if value not in expected_time_values:\n",
    "            # We have an erroneous cluster\n",
    "            time_wrong_clusters_count += 1\n",
    "        else:\n",
    "            # We have a correct cluster\n",
    "            expected_time_values.remove(value)\n",
    "            time_correct_clusters_count += 1\n",
    "\n",
    "    # Load JSON\n",
    "    with open(os.path.join(NUMBER_JSON, f'{sheet}.json')) as f:\n",
    "        number_clusters = json.load(f)\n",
    "    \n",
    "\n",
    "    # Each cluster contains the number (integer) that the cluster represents\n",
    "    # We know what integers should be represented in the time labels, lets check that they are all there.\n",
    "    # Keep track of any false positives (new clusters that don't exist) or negatives (missing clusters)\n",
    "    for cluster, value in number_clusters.items():\n",
    "        if value not in expected_number_values:\n",
    "            # We have an erroneous cluster\n",
    "            number_wrong_clusters_count += 1\n",
    "        else:\n",
    "            # We have a correct cluster\n",
    "            expected_number_values.remove(value)\n",
    "            number_correct_clusters_count += 1\n",
    "    \n",
    "\n",
    "\n",
    "print(f\"Time labels: {time_correct_clusters_count} correct clusters, {time_wrong_clusters_count} incorrect clusters. The accuracy is {time_correct_clusters_count / (time_correct_clusters_count + time_wrong_clusters_count) * 100:.2f}%\")\n",
    "print(f\"Number labels: {number_correct_clusters_count} correct clusters, {number_wrong_clusters_count} incorrect clusters. The accuracy is {number_correct_clusters_count / (number_correct_clusters_count + number_wrong_clusters_count) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Attempts\n",
    "First tried AgglomerativeClustering using a Ward hierarchical clustering algorithm with linkage distance.\n",
    "* Misjudged distance_threshold input, only takes meaningful values form 0-1\n",
    "* Best accuracy with distance_threshold = 0, 10.55% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ward linkage clustering fit and prediction\n",
    "# distance_threshold shows the limit at which to cut the dendrogram tree\n",
    "ward = AgglomerativeClustering(n_clusters=None, metric='euclidean', linkage='single', compute_full_tree=True, distance_threshold=nonmerge_threshold)\n",
    "labels = ward.fit_predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
