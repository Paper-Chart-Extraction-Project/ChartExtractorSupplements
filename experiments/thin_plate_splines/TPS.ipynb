{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thin Plate Splines Transformations\n",
    "\n",
    "**Purpose:** The purpose of this experiment to implement thin plate spline transformations on images. The goal is to help normalize images of charts that have been folded or bent. TPS registration will be performed post-homography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Thin Plate Splines\n",
    "\n",
    "For our purpose, a thin plate spline transformation can be defined as follows:  \n",
    "Given a set of \"source\" points $P_s$ and a corresponding set of \"destination\" points $P_d$, the thin plate spline transformation is a function  \n",
    "$f_{tps}(p) | \\forall p_s \\in P_s, f_{tps}$ and $f$ is the function when \"bends\" all other points the least among all differentiable functions (meaning it minimizes an energy function).\n",
    "\n",
    "![thin_plate_spline_example](https://github.com/user-attachments/assets/9f269c79-6d5d-4090-988c-39ee9e928ff0)\n",
    "\n",
    "In this image there are two sets of points which form fish shapes. The red '+' points represent the destination points, and the green 'o' points represent the source points. On the right we see the thin plate spline deformation which makes all the green points exactly match all the red points, and we can also see a grid that shows where all other points on the plane will be mapped as well.\n",
    "\n",
    "## Benefits for Image Registration\n",
    "\n",
    "While the homography is a very reliable transformation, it is limited to purely linear transformations (ex: rotation, scaling, shear).  \n",
    "Very commonly, pages will be creased or folded, the edge of pages will curl inwards or outwards, or some cameras will cause barrel/pinhole distortions.  \n",
    "This leads to a whole subset of issues that cannot be corrected linearly, but still must be accounted for.\n",
    "\n",
    "![smaller_RC_0033_preoperative_postoperative](https://github.com/user-attachments/assets/f22374e1-75b4-4a07-816f-d3ad51e84921)\n",
    "An example of a paper which has been folded. and unfolded, causing non-linear distortions. This is a _very_ modest example. In practice, we have seen photographs of papers that are raised at least an inch off the table in certain areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from utils.annotations import BoundingBox\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from scipy.interpolate import Rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for testing:\n",
    "\n",
    "- \"intraop_document_landmarks.json\"\n",
    "  - Used for the destination points in the transformation. We are using the landmarks from the unified image\n",
    "  - Also has landmarks for each of the images. We currently are not using them\n",
    "- \"yolo_data.json\"\n",
    "  - Used for the source points in the transformation.\n",
    "  - May need to be replace with the landmarks from the other file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 sheets in yolo_data.json\n"
     ]
    }
   ],
   "source": [
    "# Load yolo_data.json which will be used as src_bbs\n",
    "PATH_TO_YOLO_DATA = \"../../data/yolo_data.json\"\n",
    "PATH_TO_REGISTERED_IMAGES = \"../../data/registered_images\"\n",
    "\n",
    "with open(PATH_TO_YOLO_DATA) as json_file:\n",
    "    yolo_data = json.load(json_file)\n",
    "\n",
    "print(f\"Found {len(yolo_data)} sheets in yolo_data.json\")\n",
    "\n",
    "# load introp_document_landmarks.json which will be used as dst_points\n",
    "PATH_TO_LANDMARKS = \"../../data/intraop_document_landmarks.json\"\n",
    "\n",
    "DESIRED_IMAGE_WIDTH = 800\n",
    "DESIRED_IMAGE_HEIGHT = 600\n",
    "\n",
    "\n",
    "def label_studio_to_bboxes(path_to_json_data: Path) -> List[BoundingBox]:\n",
    "    \"\"\"\n",
    "    Convert the json data from label studio to a list of BoundingBox objects\n",
    "\n",
    "    Args:\n",
    "        path_to_json_data (Path): Path to the json data from label studio\n",
    "\n",
    "    Returns:\n",
    "        List[BoundingBox]: List of BoundingBox objects\n",
    "    \"\"\"\n",
    "    json_data: List[Dict] = json.loads(open(str(path_to_json_data)).read())\n",
    "    return {\n",
    "        sheet_data[\"data\"][\"image\"].split(\"-\")[-1]: [\n",
    "            BoundingBox(\n",
    "                category=label[\"value\"][\"rectanglelabels\"][0],\n",
    "                left=label[\"value\"][\"x\"] / 100 * DESIRED_IMAGE_WIDTH,\n",
    "                top=label[\"value\"][\"y\"] / 100 * DESIRED_IMAGE_HEIGHT,\n",
    "                right=(label[\"value\"][\"x\"] / 100 + label[\"value\"][\"width\"] / 100)\n",
    "                * DESIRED_IMAGE_WIDTH,\n",
    "                bottom=(label[\"value\"][\"y\"] / 100 + label[\"value\"][\"height\"] / 100)\n",
    "                * DESIRED_IMAGE_HEIGHT,\n",
    "            )\n",
    "            for label in sheet_data[\"annotations\"][0][\"result\"]\n",
    "        ]\n",
    "        for sheet_data in json_data\n",
    "    }\n",
    "\n",
    "\n",
    "landmark_location_data: Dict[str, List[BoundingBox]] = label_studio_to_bboxes(\n",
    "    PATH_TO_LANDMARKS\n",
    ")\n",
    "\n",
    "landmarks = landmark_location_data[\n",
    "    \"unified_intraoperative_preoperative_flowsheet_v1_1_front.png\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPS Tranformation**\n",
    "Steps:\n",
    "\n",
    "1. Filter to keep only the relevant bounding boxes\n",
    "   - remove all bounding boxes from the source points that do not match a category in the destination points\n",
    "   - Find all of the duplicates in the source and destination points and remove them\n",
    "   - sort the source and destination points alphabetically via their category\n",
    "2. Get lists of the x and y coordinates for both the source and destination points\n",
    "   - Primary purpose is to enable the use of scipy's Rbf function\n",
    "   - We are using the top left corner of the bounding boxes\n",
    "3. Estimate the transformation\n",
    "   - Use the Rbf function to apply the TPS transformation\n",
    "4. Apply the transformation and Warp the image\n",
    "   - Create a grid from 0 to maximum value of the image\n",
    "   - Apply the transformation to the grids\n",
    "   - Ensure that the transformed points are within bounds\n",
    "   - Use those grids to warp the original image\n",
    "\n",
    "_Note:_ There are a lot of different outputs that are currently commented out that can be used for debugging purposes:\n",
    "\n",
    "- Print the lists of duplicate keys and the categories being used in the source and destination points\n",
    "- Plot of the source and destination points on the image\n",
    "- View the bounds of the transformed points\n",
    "- View the distribution of the transformed points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tps_transform(\n",
    "    image: np.ndarray, src_bbs: List[BoundingBox], dst_bbs: List[BoundingBox]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a thin plate spline transformation on the image using the src_bbs and dst_bbs\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The image to be transformed\n",
    "        src_bbs (List[BoundingBox]): List of BoundingBox objects\n",
    "        dst_bbs (List[BoundingBox]): List of BoundingBox objects\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The transformed image\n",
    "    \"\"\"\n",
    "\n",
    "    # get the categories from dst_bbs\n",
    "    landmark_cats = [bb.category for bb in dst_bbs]\n",
    "    # remove all bbs in src that are not in those categories\n",
    "    src_bbs = [bb for bb in src_bbs if bb.category in landmark_cats]\n",
    "    # get list of duplicate keys\n",
    "    duplicate_count_src = dict(Counter([bb.category for bb in src_bbs]))\n",
    "    duplicates = [k for k, v in duplicate_count_src.items() if v > 1]\n",
    "    duplicate_count_dst = dict(Counter([bb.category for bb in dst_bbs]))\n",
    "    duplicates.extend([k for k, v in duplicate_count_dst.items() if v > 1])\n",
    "    duplicates = list(set(duplicates))\n",
    "    # print(duplicates)\n",
    "    # remove duplicates\n",
    "    src_bbs = [bb for bb in src_bbs if bb.category not in duplicates]\n",
    "    dst_bbs = [bb for bb in dst_bbs if bb.category not in duplicates]\n",
    "\n",
    "    # sort categories alphabetically\n",
    "    src_bbs = sorted(src_bbs, key=lambda bb: bb.category)\n",
    "    dst_bbs = sorted(dst_bbs, key=lambda bb: bb.category)\n",
    "\n",
    "    # get lists of the x and y coordinates\n",
    "    src_x, src_y = zip(*[(bb.left, bb.top) for bb in src_bbs])\n",
    "    dst_x, dst_y = zip(*[(bb.left, bb.top) for bb in dst_bbs])\n",
    "\n",
    "    # remove source points with suspiciously high distances to their destination counterparts\n",
    "    threshold = 4.0\n",
    "    # difference in source and destination x\n",
    "    delta_x = np.array(src_x) - np.array(dst_x)\n",
    "    # boolean list for source points if larger than the threshold\n",
    "    tf_x = [abs(point) < threshold for point in delta_x]\n",
    "    # difference in source and destination y\n",
    "    delta_y = np.array(src_y) - np.array(dst_y)\n",
    "    # boolean list for source points if larger than the threshold\n",
    "    tf_y = [abs(point) < threshold for point in delta_y]\n",
    "    # compare x and y boolean lists and combine to remove points with either above thershold\n",
    "    delta_combo = [\n",
    "        np.False_ if (x == np.False_ or y == np.False_) else np.True_\n",
    "        for x, y in zip(tf_x, tf_y)\n",
    "    ]\n",
    "    # remove source and destination x points if their delta is too large\n",
    "    new_src_x = [point for point, delta in zip(src_x, delta_combo) if delta == np.True_]\n",
    "    new_dst_x = [point for point, delta in zip(dst_x, delta_combo) if delta == np.True_]\n",
    "    # remove source and destination y points if their delta is too large\n",
    "    new_src_y = [point for point, delta in zip(src_y, delta_combo) if delta == np.True_]\n",
    "    new_dst_y = [point for point, delta in zip(dst_y, delta_combo) if delta == np.True_]\n",
    "\n",
    "    # plt.imshow(image)\n",
    "    # plt.scatter(src_x, src_y, color='blue', label='Source Points')\n",
    "    # plt.scatter(dst_x, dst_y, color='red', label='Destination Points')\n",
    "    # plt.legend()\n",
    "    # plt.title(\"Source and Destination Control Points\")\n",
    "    # plt.show()\n",
    "\n",
    "    # use RBF function to do the thin plate splines\n",
    "    rbf_x = Rbf(new_dst_x, new_dst_y, new_src_x, function=\"thin_plate\")\n",
    "    rbf_y = Rbf(new_dst_x, new_dst_y, new_src_y, function=\"thin_plate\")\n",
    "\n",
    "    # Alter the image according to the transformation\n",
    "    h, w, _ = image.shape\n",
    "    # create grid\n",
    "    x = np.linspace(0, w - 1, w)\n",
    "    y = np.linspace(0, h - 1, h)\n",
    "    grid_x, grid_y = np.meshgrid(x, y)\n",
    "\n",
    "    # apply the transformation\n",
    "    # reshape into grid\n",
    "    transformed_x = rbf_x(grid_x, grid_y).astype(np.float32)\n",
    "    transformed_y = rbf_y(grid_x, grid_y).astype(np.float32)\n",
    "\n",
    "    transformed_x = np.clip(transformed_x, 0, image.shape[1] - 1)\n",
    "    transformed_y = np.clip(transformed_y, 0, image.shape[0] - 1)\n",
    "\n",
    "    # print(f\"Transformed X range: {np.min(transformed_x)}, {np.max(transformed_x)}\")\n",
    "    # print(f\"Transformed Y range: {np.min(transformed_y)}, {np.max(transformed_y)}\")\n",
    "\n",
    "    # plt.imshow(transformed_x, cmap='coolwarm', interpolation='nearest')\n",
    "    # plt.title(\"Transformed X Coordinates\")\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.imshow(transformed_y, cmap='coolwarm', interpolation='nearest')\n",
    "    # plt.title(\"Transformed Y Coordinates\")\n",
    "    # plt.show()\n",
    "\n",
    "    # warp the image\n",
    "    warp_img = cv2.remap(\n",
    "        image, transformed_x, transformed_y, interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    print(type(warp_img))\n",
    "\n",
    "    return warp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "<class 'numpy.ndarray'>\n",
      "304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m sheet_bbs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     BoundingBox\u001b[38;5;241m.\u001b[39mfrom_yolo(bb, DESIRED_IMAGE_WIDTH, DESIRED_IMAGE_HEIGHT)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bb \u001b[38;5;129;01min\u001b[39;00m yolo_bbs\n\u001b[0;32m     10\u001b[0m ]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sheet_bbs))\n\u001b[1;32m---> 13\u001b[0m transformed_img \u001b[38;5;241m=\u001b[39m \u001b[43mtps_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_bbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m transformed_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(transformed_img)\n\u001b[0;32m     15\u001b[0m transformed_img\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[4], line 81\u001b[0m, in \u001b[0;36mtps_transform\u001b[1;34m(image, src_bbs, dst_bbs)\u001b[0m\n\u001b[0;32m     77\u001b[0m grid_x, grid_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(x, y)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# apply the transformation\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# reshape into grid\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m transformed_x \u001b[38;5;241m=\u001b[39m \u001b[43mrbf_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_y\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     82\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m rbf_y(grid_x, grid_y)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     84\u001b[0m transformed_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(transformed_x, \u001b[38;5;241m0\u001b[39m, image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\15406\\Coding-Projects\\Paper Chart Extraction\\Supplements\\.venv\\Lib\\site-packages\\scipy\\interpolate\\_rbf.py:290\u001b[0m, in \u001b[0;36mRbf.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    288\u001b[0m xa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([a\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    289\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_norm(xa, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxi)\n\u001b[1;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes)\u001b[38;5;241m.\u001b[39mreshape(shp)\n",
      "File \u001b[1;32mc:\\Users\\15406\\Coding-Projects\\Paper Chart Extraction\\Supplements\\.venv\\Lib\\site-packages\\scipy\\interpolate\\_rbf.py:170\u001b[0m, in \u001b[0;36mRbf._h_thin_plate\u001b[1;34m(self, r)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_h_thin_plate\u001b[39m(\u001b[38;5;28mself\u001b[39m, r):\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxlogy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sheet, yolo_bbs in yolo_data.items():\n",
    "    # get path to current image\n",
    "    full_image_path = os.path.join(PATH_TO_REGISTERED_IMAGES, sheet)\n",
    "    image = cv2.imread(full_image_path)\n",
    "    resized_img = cv2.resize(image, (DESIRED_IMAGE_WIDTH, DESIRED_IMAGE_HEIGHT))\n",
    "    # get the sheet's bounding boxes\n",
    "    sheet_bbs = [\n",
    "        BoundingBox.from_yolo(bb, DESIRED_IMAGE_WIDTH, DESIRED_IMAGE_HEIGHT)\n",
    "        for bb in yolo_bbs\n",
    "    ]\n",
    "\n",
    "    print(len(sheet_bbs))\n",
    "    transformed_img = tps_transform(resized_img, sheet_bbs, landmarks)\n",
    "    transformed_img = Image.fromarray(transformed_img)\n",
    "    transformed_img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tps_transform_ransac(image, src_bbs: List[BoundingBox], dst_bbs: List[BoundingBox]):\n",
    "    \"\"\"\n",
    "    Perform a thin plate spline transformation on the image using the src_bbs and dst_bbs, using RANSAC to filter out outliers.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The image to be transformed\n",
    "        src_bbs (List[BoundingBox]): List of BoundingBox objects\n",
    "        dst_bbs (List[BoundingBox]): List of BoundingBox objects\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The transformed image\n",
    "    \"\"\"\n",
    "    # get the categories from dst_bbs\n",
    "    landmark_cats = [bb.category for bb in dst_bbs]\n",
    "    # remove all bbs in src that are not in those categories\n",
    "    src_bbs = [bb for bb in src_bbs if bb.category in landmark_cats]\n",
    "    # get list of duplicate keys\n",
    "    duplicate_count_src = dict(Counter([bb.category for bb in src_bbs]))\n",
    "    duplicates = [k for k, v in duplicate_count_src.items() if v > 1]\n",
    "    duplicate_count_dst = dict(Counter([bb.category for bb in dst_bbs]))\n",
    "    duplicates.extend([k for k, v in duplicate_count_dst.items() if v > 1])\n",
    "    duplicates = list(set(duplicates))\n",
    "    # print(duplicates)\n",
    "    # remove duplicates\n",
    "    src_bbs = [bb for bb in src_bbs if bb.category not in duplicates]\n",
    "    dst_bbs = [bb for bb in dst_bbs if bb.category not in duplicates]\n",
    "    # sort categories alphabetically\n",
    "    src_bbs = sorted(src_bbs, key=lambda bb: bb.category)\n",
    "    # print([bb.category for bb in src_bbs])\n",
    "    dst_bbs = sorted(dst_bbs, key=lambda bb: bb.category)\n",
    "    # print([bb.category for bb in dst_bbs])\n",
    "\n",
    "    src_bbs = np.array([[bb.left, bb.top] for bb in src_bbs], dtype=np.float32)\n",
    "    dst_bbs = np.array([[bb.left, bb.top] for bb in dst_bbs], dtype=np.float32)\n",
    "\n",
    "    _, mask = cv2.findHomography(\n",
    "        dst_bbs,\n",
    "        src_bbs,\n",
    "        method=cv2.RANSAC,\n",
    "        ransacReprojThreshold=10.0,\n",
    "        maxIters=5000,\n",
    "        confidence=0.99,\n",
    "    )\n",
    "    inlier_mask = mask.ravel() == 1\n",
    "\n",
    "    filtered_src = src_bbs[inlier_mask]\n",
    "    filtered_dst = dst_bbs[inlier_mask]\n",
    "\n",
    "    new_src_x, new_src_y = filtered_src[:, 0], filtered_src[:, 1]\n",
    "    new_dst_x, new_dst_y = filtered_dst[:, 0], filtered_dst[:, 1]\n",
    "\n",
    "    if len(new_src_x) < 4:\n",
    "        return image\n",
    "\n",
    "    # use RBF function to do the thin plate splines\n",
    "    rbf_x = Rbf(new_dst_x, new_dst_y, new_src_x, function=\"thin_plate\")\n",
    "    rbf_y = Rbf(new_dst_x, new_dst_y, new_src_y, function=\"thin_plate\")\n",
    "\n",
    "    # Alter the image according to the transformation\n",
    "    h, w, _ = image.shape\n",
    "    # create grid\n",
    "    x = np.linspace(0, w - 1, w)\n",
    "    y = np.linspace(0, h - 1, h)\n",
    "    grid_x, grid_y = np.meshgrid(x, y)\n",
    "\n",
    "    # apply the transformation\n",
    "    # reshape into grid\n",
    "    transformed_x = rbf_x(grid_x, grid_y).astype(np.float32)\n",
    "    transformed_y = rbf_y(grid_x, grid_y).astype(np.float32)\n",
    "\n",
    "    transformed_x = np.clip(transformed_x, 0, image.shape[1] - 1)\n",
    "    transformed_y = np.clip(transformed_y, 0, image.shape[0] - 1)\n",
    "\n",
    "    # warp the image\n",
    "    warp_img = cv2.remap(\n",
    "        image, transformed_x, transformed_y, interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    return warp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet, yolo_bbs in yolo_data.items():\n",
    "    # get path to current image\n",
    "    full_image_path = os.path.join(PATH_TO_REGISTERED_IMAGES, sheet)\n",
    "    image = cv2.imread(full_image_path)\n",
    "    resized_img = cv2.resize(image, (DESIRED_IMAGE_WIDTH, DESIRED_IMAGE_HEIGHT))\n",
    "    # get the sheet's bounding boxes\n",
    "    sheet_bbs = [\n",
    "        BoundingBox.from_yolo(bb, DESIRED_IMAGE_WIDTH, DESIRED_IMAGE_HEIGHT)\n",
    "        for bb in yolo_bbs\n",
    "    ]\n",
    "    transformed_img = tps_transform_ransac(resized_img, sheet_bbs, landmarks)\n",
    "    transformed_img = Image.fromarray(transformed_img)\n",
    "    transformed_img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging these two methods and priming for placing into extractor\n",
    "\n",
    "I'm going to create private functions that do RANSAC and distance measurments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with turning Hannah's code into a function\n",
    "def __filter_by_distance(\n",
    "    src_bbs: List[BoundingBox],\n",
    "    dst_bbs: List[BoundingBox],\n",
    "    threshold: float,\n",
    ") -> Tuple[List[BoundingBox], List[BoundingBox]]:\n",
    "    \"\"\"\n",
    "    Filter out source and destination points that have a distance greater than the threshold.\n",
    "    Large transformations are likely to be outliers and erroneous. We only expect small tweaks via the thin plate spline transformation.\n",
    "    Homography should already be completed prior to TPS.\n",
    "\n",
    "    Args:\n",
    "        src_bbs (List[BoundingBox]): The source points\n",
    "        dst_bbs (List[BoundingBox]): The destination points\n",
    "        threshold (float): The threshold distance\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[BoundingBox], List[BoundingBox]]: The filtered source and destination points\n",
    "\n",
    "    \"\"\"\n",
    "    filtered_points = [\n",
    "        (src_bb, dst_bb)\n",
    "        for src_bb, dst_bb in zip(src_bbs, dst_bbs)\n",
    "        if abs(src_bb.top - dst_bb.top) < threshold\n",
    "        and abs(src_bb.left - dst_bb.left) < threshold\n",
    "    ]\n",
    "\n",
    "    new_src_bbs, new_dst_bbs = zip(*filtered_points) if filtered_points else ([], [])\n",
    "\n",
    "    return list(new_src_bbs), list(new_dst_bbs)\n",
    "\n",
    "\n",
    "# Now we can turn Matt's RANSAC code into a function\n",
    "def __filter_by_RANSAC(\n",
    "    src_bbs: List[BoundingBox],\n",
    "    dst_bbs: List[BoundingBox],\n",
    "    threshold: float,\n",
    "    max_iters: int = 5000,\n",
    "    confidence_limit: float = 0.99,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Filter out source and destination points that are not inliers according to RANSAC\n",
    "\n",
    "    Args:\n",
    "        src_bbs (np.ndarray): The source points\n",
    "        dst_bbs (np.ndarray): The destination points\n",
    "        threshold (float): The threshold distance\n",
    "        max_iters (int, optional): The maximum number of iterations for RANSAC. Defaults to 5000.\n",
    "        confidence_limit (float, optional): The confidence limit for RANSAC. Defaults to 0.99.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: The filtered source and destination points as numpy arrays in this order (src_x, src_y, dst_x, dst_y)\n",
    "    \"\"\"\n",
    "    # Turn the BoundingBox objects into numpy arrays of coordinates\n",
    "    src_points = np.array([[bb.left, bb.top] for bb in src_bbs], dtype=np.float32)\n",
    "    dst_points = np.array([[bb.left, bb.top] for bb in dst_bbs], dtype=np.float32)\n",
    "\n",
    "    # Complete RANSAC\n",
    "    _, mask = cv2.findHomography(\n",
    "        dst_points,\n",
    "        src_points,\n",
    "        method=cv2.RANSAC,\n",
    "        ransacReprojThreshold=threshold,\n",
    "        maxIters=max_iters,\n",
    "        confidence=confidence_limit,\n",
    "    )\n",
    "    inlier_mask = mask.ravel() == 1\n",
    "\n",
    "    # Apply the mask to the source and destination points\n",
    "    filtered_src = src_points[inlier_mask]\n",
    "    filtered_dst = dst_points[inlier_mask]\n",
    "\n",
    "    # Get the x and y coordinates of the filtered points\n",
    "    src_x, src_y = filtered_src[:, 0], filtered_src[:, 1]\n",
    "    dst_x, dst_y = filtered_dst[:, 0], filtered_dst[:, 1]\n",
    "\n",
    "    return src_x, src_y, dst_x, dst_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to merge the two functions into one that uses RANSAC to filter out outliers as well as those far from the destination points\n",
    "def transform_thin_plate_splines(\n",
    "    image: np.ndarray,\n",
    "    src_bbs: List[BoundingBox],\n",
    "    dst_bbs: List[BoundingBox],\n",
    "    max_dist: float = 4.0,\n",
    "    threshold: float = 10.0,\n",
    "    max_iters: int = 5000,\n",
    "    confidence_limit: float = 0.99,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a thin plate spline transformation on the image using the src_bbs and dst_bbs, using RANSAC to filter out outliers.\n",
    "    We assume that homography was completed prior to calling this function.\n",
    "    We start by filtering by points that are too far from their destination counterparts, then we use RANSAC to filter out outliers.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The image to be transformed\n",
    "        src_bbs (List[BoundingBox]): List of BoundingBox objects\n",
    "        dst_bbs (List[BoundingBox]): List of BoundingBox objects\n",
    "        max_dist (float, optional): The maximum distance for filtering out points. Defaults to 4.0.\n",
    "        threshold (float, optional): The threshold distance for RANSAC. Defaults to 10.0.\n",
    "        max_iters (int, optional): The maximum number of iterations for RANSAC. Defaults to 5000.\n",
    "        confidence_limit (float, optional): The confidence limit for RANSAC. Defaults to 0.99.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The transformed image\n",
    "    \"\"\"\n",
    "    # get the categories from dst_bbs\n",
    "    landmark_cats = [bb.category for bb in dst_bbs]\n",
    "    # remove all bbs in src that are not in those categories\n",
    "    src_bbs = [bb for bb in src_bbs if bb.category in landmark_cats]\n",
    "    # get list of duplicate keys\n",
    "    duplicate_count_src = dict(Counter([bb.category for bb in src_bbs]))\n",
    "    duplicates = [k for k, v in duplicate_count_src.items() if v > 1]\n",
    "    duplicate_count_dst = dict(Counter([bb.category for bb in dst_bbs]))\n",
    "    duplicates.extend([k for k, v in duplicate_count_dst.items() if v > 1])\n",
    "    duplicates = list(set(duplicates))\n",
    "    # print(duplicates)\n",
    "    # remove duplicates\n",
    "    src_bbs = [bb for bb in src_bbs if bb.category not in duplicates]\n",
    "    dst_bbs = [bb for bb in dst_bbs if bb.category not in duplicates]\n",
    "\n",
    "    # sort categories alphabetically\n",
    "    src_bbs = sorted(src_bbs, key=lambda bb: bb.category)\n",
    "    dst_bbs = sorted(dst_bbs, key=lambda bb: bb.category)\n",
    "\n",
    "    # remove source points with suspiciously high distances to their destination counterparts\n",
    "    src_bbs, dst_bbs = __filter_by_distance(src_bbs, dst_bbs, max_dist)\n",
    "\n",
    "    # Now lets use RAANSAC to filter out outliers\n",
    "    src_x, src_y, dst_x, dst_y = __filter_by_RANSAC(\n",
    "        src_bbs, dst_bbs, threshold, max_iters, confidence_limit\n",
    "    )\n",
    "\n",
    "    # use RBF function to do the thin plate splines\n",
    "    rbf_x = Rbf(dst_x, dst_y, src_x, function=\"thin_plate\")\n",
    "    rbf_y = Rbf(dst_x, dst_y, src_y, function=\"thin_plate\")\n",
    "\n",
    "    # Alter the image according to the transformation\n",
    "    h, w, _ = image.shape\n",
    "    # create grid\n",
    "    x = np.linspace(0, w - 1, w)\n",
    "    y = np.linspace(0, h - 1, h)\n",
    "    grid_x, grid_y = np.meshgrid(x, y)\n",
    "\n",
    "    # apply the transformation\n",
    "    # reshape into grid\n",
    "    transformed_x = rbf_x(grid_x, grid_y).astype(np.float32)\n",
    "    transformed_y = rbf_y(grid_x, grid_y).astype(np.float32)\n",
    "\n",
    "    transformed_x = np.clip(transformed_x, 0, image.shape[1] - 1)\n",
    "    transformed_y = np.clip(transformed_y, 0, image.shape[0] - 1)\n",
    "\n",
    "    # warp the image\n",
    "    warp_img = cv2.remap(\n",
    "        image, transformed_x, transformed_y, interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    return warp_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet, yolo_bbs in yolo_data.items():\n",
    "    # get path to current image\n",
    "    full_image_path = os.path.join(PATH_TO_REGISTERED_IMAGES, sheet)\n",
    "    image = cv2.imread(full_image_path)\n",
    "    resized_img = cv2.resize(image, (DESIRED_IMAGE_WIDTH, DESIRED_IMAGE_HEIGHT))\n",
    "    # get the sheet's bounding boxes\n",
    "    sheet_bbs = [\n",
    "        BoundingBox.from_yolo(bb, DESIRED_IMAGE_WIDTH, DESIRED_IMAGE_HEIGHT)\n",
    "        for bb in yolo_bbs\n",
    "    ]\n",
    "    transformed_img = transform_thin_plate_splines(resized_img, sheet_bbs, landmarks)\n",
    "    transformed_img = Image.fromarray(transformed_img)\n",
    "    transformed_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
